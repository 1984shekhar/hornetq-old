<?xml version="1.0" encoding="UTF-8"?>
<!-- ============================================================================= -->
<!-- Copyright © 2009 Red Hat, Inc. and others.                                    -->
<!--                                                                               -->
<!-- The text of and illustrations in this document are licensed by Red Hat under  -->
<!-- a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). -->
<!--                                                                               -->
<!-- An explanation of CC-BY-SA is available at                                    -->
<!--                                                                               -->
<!--            http://creativecommons.org/licenses/by-sa/3.0/.                    -->
<!--                                                                               -->
<!-- In accordance with CC-BY-SA, if you distribute this document or an adaptation -->
<!-- of it, you must provide the URL for the original version.                     -->
<!--                                                                               -->
<!-- Red Hat, as the licensor of this document, waives the right to enforce,       -->
<!-- and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent        -->
<!-- permitted by applicable law.                                                  -->
<!-- ============================================================================= -->
<chapter id="ha">
    <title>High Availability and Failover</title>
    <para>We define high availability as the <emphasis>ability for the system to continue
            functioning after failure of one or more of the servers</emphasis>. A part of high
        availability is <emphasis>failover</emphasis> which we define as the <emphasis>ability for
            client connections to migrate from one server to another in event of server failure so
            client applications can continue to operate</emphasis>.</para>
    <para>HornetQ provides high availability by replicating servers in pairs. It also provides both
        client failover and application-level client failover.</para>
    <section>
        <title>Live - Backup Pairs</title>
        <para>HornetQ allows pairs of servers to be linked together as <emphasis>live -
                backup</emphasis> pairs. In this release there is a single backup server for each
            live server. Backup servers are not operational until failover occurs. In later releases
            we will most likely support replication onto multiple backup servers.</para>
       <para>Before failover, only the live server is serving the HornetQ clients while the backup server remains passive.
          When clients fail over to the backup server, the backup server becomes active and start to service the HornetQ clients.</para>
        
        <section id="ha.mode">
          <title>HA modes</title>
          <para>HornetQ provides two different modes for High Availability, either by <emphasis>replicating data</emphasis> from the live server journal 
             to the backup server or using a <emphasis>shared state</emphasis> for both servers.</para>
          <section id="ha.mode.replicated">
             <title>Data Replication</title>
             <para>In this mode, data stored in HornetQ journal are replicated from the live servers's journal to the
                backuper server's journal.</para>
             <para>Replication is performed in an asynchronous fashion between live and backup server.
                 Data is replicated one way in a stream, and responses that the data has reached the
                 backup is returned in another stream. Pipelining replications and responses to
                 replications in separate streams allows replication throughput to be much higher than if
                 we synchronously replicated data and waited for a response serially in an RPC manner
                 before replicating the next piece of data.</para>
             <graphic fileref="images/ha-replicated-store.png" align="center"/>
             <section id="configuring.live.backup">
                <title>Configuration</title>
                <para>First, on the live server, in <literal>hornetq-configuration.xml</literal>, 
                  configure the live server with knowledge of its backup server. This is done by
                  specifying a <literal>backup-connector-ref</literal> element. This element
                  references a connector, also specified on the live server which contains knowledge
                  of how to connect to the backup server.</para>
               <para>Here's a snippet from live server's <literal
                      >hornetq-configuration.xml</literal> configured to connect to its backup server:</para>
              <programlisting>
  &lt;backup-connector-ref connector-name="backup-connector"/>

  &lt;connectors>
     &lt;!-- This connector specifies how to connect to the backup server    -->
     &lt;!-- backup server is located on host "192.168.0.11" and port "5445" -->
     &lt;connector name="backup-connector">
       &lt;factory-class>org.hornetq.integration.transports.netty.NettyConnectorFactory&lt;/factory-class>
       &lt;param key="host" value="192.168.0.11"/>
       &lt;param key="port" value="5445"/>
     &lt;/connector>
  &lt;/connectors></programlisting>
              <para>Secondly, on the backup server, we flag the server as a backup and make sure it has an acceptor that the live server can connect to:</para>
              <programlisting>
  &lt;backup>true&lt;/backup>

  &lt;acceptors>
     &lt;acceptor name="acceptor">
        &lt;factory-class>org.hornetq.integration.transports.netty.NettyAcceptorFactory&lt;/factory-class>
        &lt;param key="host" value="192.168.0.11"/>
        &lt;param key="port" value="5445"/>
     &lt;/acceptor>
  &lt;/acceptors>               
              </programlisting>
              <para>For a backup server to function correctly it's also important that it has the same
                  set of bridges, predefined queues, cluster connections, broadcast groups and
                  discovery groups as defined on the live node. The easiest way to ensure this is just
                  to copy the entire server side configuration from live to backup and just make the
                  changes as specified above. </para>
          </section>
             <section>
                 <title>Synchronization of live-backup pairs</title>
                 <para>In order for live - backup pairs to operate properly, they must be identical
                     replicas. This means you cannot just use any backup server that's previously been
                     used for other purposes as a backup server, since it will have different data in its
                     persistent storage. If you try to do so you will receive an exception in the logs
                     and the server will fail to start.</para>
                 <para>To create a backup server for a live server that's already been used for other
                     purposes, it's necessary to copy the <literal>data</literal> directory from the live
                     server to the backup server. This means the backup server will have an identical
                     persistent store to the backup server.</para>
                 <para>After failover, when the live server is restarted, the backup server will copy its
                    journal back to the live server. When the live server has the updated journal, it will
                    become active again and the backup server will become passive.</para>
             </section>
          </section>
          <section id="ha.mode.shared">
             <title>Shared Store</title>
             <para>When using a shared store, both live and backup servers share the <emphasis>same</emphasis> journal
             using a shared file system. When failover occurs and backup server takes over, it will load the journal and
             clients can connect to it.</para>
             <graphic fileref="images/ha-shared-store.png" align="center"/>
             <section id="ha/mode.shared.configuration">
                <title>Configuration</title>
                <para>To configure the live and backup server to share their store, configure both <literal>hornetq-configuration.xml</literal>:</para>
                <programlisting>
                   &lt;shared-store>true&lt;shared-store>
                </programlisting>
                <para>In order for live - backup pairs to operate properly with a shared store, both servers
                   must have configured the location of journal directory to point
                        to the <emphasis>same shared location</emphasis> (as explained in <xref linkend="configuring.message.journal" />)</para>
               <para>If clients will use automatic failover with JMS, the live server will need to configure a connector
                  to the backup server and reference it from its <literal>hornetq-jms.xml</literal> configuration as explained
                  in <xref linkend="ha.automatic.failover" />.</para>
             </section>
             <section>
                 <title>Synchronization of live-backup pairs</title>
                 <para>As both live and backup servers share the same journal, they do not need to be synchronized.
                    However until, both live and backup servers are up and running, high-availability can not be provided with a single server.
                    After failover, at first opportunity, stop the backup server (which is active) and restart the live and backup servers.</para>
             </section>
          </section>
        </section>
    </section>
    
    <section id="failover">
      <title>Failover Modes</title>
      <para>HornetQ defines 3 types of failover:</para>
      <itemizedlist>
         <listitem><para>100% transparent re-attach to a single server as explained in <xref linkend="client-reconnection" /></para></listitem>
         <listitem><para>automatic failover</para></listitem>
         <listitem><para>application-level failover</para></listitem>
      </itemizedlist>
      
    <section id="ha.automatic.failover">
        <title>Automatic Client Failover</title>
        <para>HornetQ clients can be configured with knowledge of live and backup servers, so that
            in event of connection failure of the client - live server connection, the client will
            detect this and reconnect to the backup server. The backup server will have recreated the sessions
            and consumers but it will not preserve the session state from the live server.</para>
        <para>HornetQ clients detect connection failure when it has not received packets from the
            server within the time given by <literal>client-failure-check-period</literal> as
            explained in section <xref linkend="connection-ttl"/>. If the client does not receive
            data in good time, it will assume the connection has failed and attempt failover.</para>
        <para>HornetQ clients can be configured with the list of live-backup server pairs in a
            number of different ways. They can be configured explicitly or probably the most common
            way of doing this is to use <emphasis>server discovery</emphasis> for the client to
            automatically discover the list. For full details on how to configure server discovery, please see
                <xref linkend="clusters.server-discovery"/>. Alternatively, the clients can  explicitely specifies pairs of
                live-backup server as explained in <xref linkend="clusters.static.servers" />.</para>
        <para>To enable automatic client failover, the client must be configured to allow non-zero reconnection attempts
           (as explained in <xref linkend="client-reconnection" />).</para>
        <para>Sometimes you want a client to failover onto a backup server even if the live server
            is just cleanly shutdown rather than having crashed or the connection failed. To
            configure this you can set the property <literal>FailoverOnServerShutdown</literal> to
            false either on the <literal>HornetQConnectionFactory</literal> if you're using JMS or
            in the <literal>hornetq-jms.xml</literal> file when you define the connection factory,
            or if using core by setting the property directly on the <literal
                >ClientSessionFactoryImpl</literal> instance after creation. The default value for
            this property is <literal>false</literal>, this means that by default <emphasis>HornetQ
                clients will not failover to a backup server if the live server is simply shutdown
                cleanly.</emphasis></para>
        <para>For examples of automatic failover with transacted and non-transacted JMS sessions, please see <xref
                    linkend="examples.transaction-failover"/> and <xref linkend="examples.non-transaction-failover" />.</para>    </section>
    <section>
        <title>Application-Level Failover</title>
        <para>In some cases you may not want automatic client failover, and prefer to handle any
            connection failure yourself, and code your own manually reconnection logic in your own
            failure handler. We define this as <emphasis>application-level</emphasis> failover,
            since the failover is handled at the user application level.</para>
        <para>If all your clients use application-level failover then you do not need data
            replication on the server side, and should disabled this. Server replication has some
            performance overhead and should be disabled if it is not required. To disable server
            replication simply do not specify a <literal>backup-connector</literal> element on each
            live server.</para>
        <para>To implement application-level failover, if you're using JMS then you need to code an
                <literal>ExceptionListener</literal> class on the JMS connection. The <literal
                >ExceptionListener</literal> will be called by HornetQ in the event that connection
            failure is detected. In your <literal>ExceptionListener</literal> you would close your
            old JMS connections, potentially look up new connection factory instances from JNDI and
            creating new connections. In this case you may well be using <ulink
                url="http://www.jboss.org/community/wiki/JBossHAJNDIImpl">HA-JNDI</ulink> to ensure
            that the new connection factory is looked up from a different server.</para>
        <para>For a working example of application-level failover, please see <xref
                linkend="application-level-failover"/>.</para>
        <para>If you are using the core API, then the procedure is very similar: you would code a
                <literal>FailureListener</literal> on your core <literal>ClientSession</literal>
            instances.</para>
    </section>
    </section>
</chapter>
